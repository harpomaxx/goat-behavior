---
title: "SHAP Values for goat behavior"
output: html_notebook
---

```{r eval=FALSE, include=FALSE}
library(shapper)
```
```{r eval=FALSE, include=FALSE}
library("titanic")
titanic <- titanic_train[,c("Survived", "Pclass", "Age", "SibSp", "Parch", "Fare")]
titanic$Survived <- factor(titanic$Survived)
titanic$Sex <- factor(titanic$Sex)
titanic$Embarked <- factor(titanic$Embarked)
titanic <- na.omit(titanic)
head(titanic)
```
```{r eval=FALSE, include=FALSE}
library("randomForest")
set.seed(123)
model_rf <- randomForest(Survived ~ . , data = titanic)
model_rf
```
```{r eval=FALSE, include=FALSE}
new_passanger <- data.frame(
            Pclass = 1,
            Sex = factor("male", levels = c("female", "male")),
            Age = 8,
            SibSp = 0,
            Parch = 0,
            Fare = 72,
            Embarked = factor("C", levels = c("","C","Q","S"))
)
predict(model_rf, new_passanger, type = "prob")
```
```{r eval=FALSE, include=FALSE}
library("DALEX")
exp_rf <- explain(model_rf, data = titanic[,-1])
```

# Goat behavior
## Shapper package (not used)
```{r eval=FALSE, include=FALSE}
library("shapper")
p_function <- function(model, data) predict(model, newdata = data, type = "prob")
   

#ive_rf <- individual_variable_effect(model_rf, data = titanic[,-1], predict_function = p_function,
#            new_observation = titanic[1:2,-1], nsamples = 50)

train<-train_dataset %>% 
                                                select(selected_variables$variable) %>%  
                                                na.omit() %>% 
                                                as.data.frame()
#names(train)[4]<-"distance"
#names(train)[8]<-"%headdown"

ive_rf <- individual_variable_effect(boostFit, 
                                     data = train , 
                                     predict_function = p_function,
                                     
                                 #   new_observation = train[which(train_dataset$Activity=="R"),], 
                                 
                                    new_observation = train[1:3,], 
            
                                    nsamples = 200)

ive_rf %>% as.data.frame() %>% filter(`_id_`==1)  %>% group_by(`_vname_`) %>% summarise(n=n())

ive_rf %>% as.data.frame()

```
```{r eval=FALSE, fig.width=18, include=FALSE}
#plot.individual_variable_effect(ive_rf)
plot(ive_rf, show_predicted = TRUE, bar_width = 4)
ggsave()
```
```{r}
plot.individual_variable_effect <-
  function(x,
           ...,
           id = 1,
           digits = 2,
           rounding_function = round,
           show_predicted = TRUE,
           show_attributions = TRUE,
           cols = c("label", "id"),
           rows = "ylevel",
           selected = NULL,
           bar_width = 8,
           vcolors = c(
             `-` = "#f05a71",
             `0` = "#371ea3",
             `+` = "#8bdcbe",
             X = "#371ea3",
             pred = "#371ea3"
           )) {
    `_id_` <-
      `_attribution_` <- `_sign_` <- `_vname_` <- `_varvalue_` <- NULL
    `_yhat_mean_` <- `_yhat_` <- `_ext_vname_` <- `pretty_text` <-
      NULL

    dfl <- c(list(x), list(...))
    x <- do.call(rbind, dfl)
    class(x) <- "data.frame"
   
    # if selected is specified then select only these classess
    if (!is.null(selected)) {
      x <- x[x$`_ylevel_` %in% selected, ]
    }

    # if id is specified then select only these observations
    x <- x[x$`_id_` %in% id, ]
    values <-
      as.vector(x[1 , x$`_vname_`[1:(length(unique(x$`_vname_`)) * length(id))]])
    names(values) <- unique(paste(x$`_vname_`, x$`_id_`))

    
    
    for (i in 1:length(values)) {
      variable_i <- sub(" .*", "", names(values)[i])
     # id_i <- sub(".* ", "", names(values)[i])
      id_i <- "1"

      values[i] <-
        x[x$`_vname_` == variable_i & x$`_id_` == id_i, ][1, variable_i]
 
    }
    variable_values <- values[paste(x$`_vname_`, x$`_id_`)]
    numeric_values <- sapply(variable_values, is.numeric)
    variable_values[numeric_values] <-
      rounding_function(variable_values[numeric_values], digits)
    x$`_varvalue_` <- t(variable_values)
    x$`_vname_` <-
      reorder(x$`_vname_`, x$`_attribution_`, function(z)
        sum(abs(z)))
    x$`_ext_vname_` <- paste(x$`_vname_`, "=", x$`_varvalue_`)
    x$`_ext_vname_` <-
      reorder(x$`_ext_vname_`, as.numeric(x$`_vname_`) * 0.001 + x$`_id_`, function(z)
        sum(z))
    x$`_vname_id_` <- paste(x$`_id_`, x$`_vname_`)
  
    x$pretty_text <-
      paste0("   ", rounding_function(x$`_attribution_`, digits), "   ")
    if (show_predicted == TRUE) {
      levels(x$`_ext_vname_`) <- c(levels(x$`_ext_vname_`), "_predicted_")
      for (i in 1:length(id)) {
        x_pred <- x[x$`_id_`  == id[i], ]
        x_pred$`_ext_vname_` <-
          factor("_predicted_", levels = levels(x$`_ext_vname_`))
        x_pred$`_attribution_` <-
          x_pred$`_yhat_` - x_pred$`_yhat_mean_`
        x_pred$pretty_text <-
          paste0("   ", rounding_function(x_pred$`_yhat_`, digits), "   ")
        x_pred$`_sign_` <- "pred"
        x <- rbind(x, x_pred)
      }
    }

    rows <- paste(paste0("`_", rows, "_`"), collapse = "+")
    cols <- paste(paste0("`_", cols, "_`"), collapse = "+")
    grid_formula <- as.formula(paste(rows, "~", cols))

    id_labeller <- function(value)
      paste0("id = ", value)
    label_labeller <- function(value) {
      if (length(unique(x$`_label_`)) > 1)
        return(value)
      ""
    }
 
    pl <- ggplot(
      x,
      aes(
        x = `_ext_vname_`,
        y = `_yhat_mean_` + pmax(`_attribution_`, 0),
        ymin = `_yhat_mean_`,
        ymax = `_yhat_mean_` + `_attribution_`,
        color = `_sign_`
      )
    ) +
      geom_linerange(size = bar_width) +
      geom_hline(aes(yintercept = `_yhat_mean_`), color = "#371ea3") +
      facet_grid(grid_formula,
                 labeller = labeller(
                   `_id_` = as_labeller(id_labeller),
                   `_label_` = as_labeller(label_labeller)
                 )) +
      scale_color_manual(values =  vcolors) +
      coord_flip() + DALEX::theme_drwhy_vertical() + theme(legend.position = "none") +
      xlab("") + ylab("Shapley values") + ggtitle("")

    if (show_attributions) {
      pl <- pl + geom_text(aes(label = pretty_text), hjust = 0)
    }

    pl

  }
```

## FASTSHAP

```{r eval=FALSE, include=FALSE}
# Compute approximate Shapley values using 10 Monte Carlo simulations
set.seed(101)  # for reproducibility
library("titanic")
library("randomForest")

titanic <- titanic_train[,c("Survived", "Pclass", "Age", "SibSp", "Parch", "Fare")]
titanic$Survived <- factor(titanic$Survived)
titanic$Sex <- factor(titanic$Sex)
titanic$Embarked <- factor(titanic$Embarked)
titanic <- na.omit(titanic)

set.seed(123)
model_rf <- randomForest(Survived ~ . , data = titanic)
model_rf

predict(model_rf,titanic,type="prob")[,2]
predict_rf<-function(model,newdata) predict(model, newdata=newdata, type='prob')[,2]

shap <- fastshap::explain(model_rf, X = subset(titanic, select = -Survived), nsim = 10,
                pred_wrapper = predict_rf)
shap[1,] %>% sum()
```


```{r}
# Load required packages
library(dplyr)
library(doMC)
library(fastshap)  # for fast (approximate) Shapley values
#library(ranger)    # for fast random forest algorithm

# Simulate training data
#trn <- gen_friedman(3000, seed = 101)
#X <- subset(trn, select = -y)  # feature columns only
p_function_G<- function(object, newdata) caret::predict.train(object, newdata = newdata, type = "prob")[,"G"]
p_function_GM<- function(object, newdata) caret::predict.train(object, newdata = newdata, type = "prob")[,"GM"]
#p_function_RS<- function(object, newdata) caret::predict.train(object, newdata = newdata, type = "prob")[,"RS"]
#p_function_RL<- function(object, newdata) caret::predict.train(object, newdata = newdata, type = "prob")[,"RL"]
p_function_R<- function(object, newdata) caret::predict.train(object, newdata = newdata, type = "prob")[,"R"]
p_function_W<- function(object, newdata) caret::predict.train(object, newdata = newdata, type = "prob")[,"W"]

trainset<-train_dataset %>% 
            select(selected_variables$variable) %>% #select(-prevActivity1,-prevActivity2,-prevActivity3) %>%
            na.omit() %>% 
            as.data.frame()
trainset_y<-train_dataset %>% 
            select(Activity) %>%
            na.omit() %>% unlist() %>% unname()


trainset_G<-trainset[which(trainset_y=="G"),]
trainset_GM<-trainset[which(trainset_y=="GM"),]
#trainset_RS<-trainset[which(trainset_y=="RS"),]
#trainset_RL<-trainset[which(trainset_y=="RL"),]
trainset_R<-trainset[which(trainset_y=="R"),]
trainset_W<-trainset[which(trainset_y=="W"),]

                    
#ranks<-rank(-table(trainset_y), ties.method="first")
#trainset_y<-ranks[trainset_y] %>% unname()
#readr::write_csv(cbind(trainset,Activity=trainset_y),"goat-dataset-example.csv")
#caret::predict.train(boostFit, newdata = trainset, type='prob')[,"RL"]
# Compute fast (approximate) Shapley values using 10 Monte Carlo repetitions
shap_values_G <- fastshap::explain(boostFit, X = trainset, pred_wrapper = p_function_G, nsim = 50,newdata=trainset_G)
shap_values_GM <- fastshap::explain(boostFit, X = trainset, pred_wrapper = p_function_GM, nsim = 50,newdata=trainset_GM)
#shap_values_RS<- fastshap::explain(boostFit, X = trainset_RS, pred_wrapper = p_function_RS, nsim = 10)
#shap_values_RL <- fastshap::explain(boostFit, X = trainset_RL, pred_wrapper = p_function_RL, nsim = 10)
shap_values_R<- fastshap::explain(boostFit, X = trainset, pred_wrapper = p_function_R, nsim = 50,newdata=trainset_R)
shap_values_W <- fastshap::explain(boostFit, X = trainset, pred_wrapper = p_function_W, nsim = 50,newdata=trainset_W,adjust=TRUE)

shap_values_W[8,] %>% sum()+0.145
p_function_W(boostFit,trainset_W[8,])

p1<-autoplot(shap_values_G[13,], type = "contribution")+ggdark::dark_theme_classic()

p2 <- autoplot(shap_values_G, type = "dependence", feature = "Active", X = trainset_G, alpha = 0.5,
               color_by = "Active", smooth = TRUE, smooth_color = "orange") +
        scale_color_viridis_c()
gridExtra::grid.arrange(p1, p2+ggdark::dark_theme_classic(), nrow = 1)

#ggsave("/tmp/shap-value-one-instance.png",height = 3, width = 4,units = 'in')

#trainset_melted %>% group_by(class) %>% summarise(n=n()) %>% summarise(class=class,percent=n/sum(n))
#ggsave(p1p2,"/tmp/contribution-dependecy.png",height = 3, width = 4,units = 'in')
#ggsave("/tmp/shap-basic.png",height = 3, width = 4,units = 'in')

```
### Force plots
```{r}
force_plot(object = shap_values_W[8L,], feature_values = trainset_W[8L, ], display = "html",baseline = 0.14) 
```

```{r}
trainset_GM$class<-"GM"
trainset_G$class<-"G"
#trainset_RL$class<-"RL"
#trainset_RS$class<-"RS"
trainset_R$class<-"R"
trainset_W$class<-"W"

trainset_G  <- trainset_G %>% reshape2::melt() 
trainset_GM <- trainset_GM %>% reshape2::melt() 
#trainset_RL <- trainset_RL %>% reshape2::melt() 
#trainset_RS <- trainset_RS %>% reshape2::melt() 
trainset_R <- trainset_R %>% reshape2::melt() 
trainset_W <- trainset_W %>% reshape2::melt() 

trainset_melted<-rbind(trainset_G,
                   trainset_GM
                  # trainset_RL
                  #,trainset_RS
                  ,trainset_R
                  ,trainset_W
                   
                   )

range01 <- function(x){(x-min(x))/(max(x)-min(x))}
trainset_melted_scaled<-trainset_melted %>% group_by(variable) %>% mutate(value_scale=range01(value))
```


```{r}
shap_values_GM$class<-"GM"
shap_values_G$class<-"G"
#shap_values_RL$class<-"RL"
#shap_values_RS$class<-"RS"
shap_values_R$class<-"R"
shap_values_W$class<-"W"

shap_value_G  <- shap_values_G %>% reshape2::melt() 
shap_value_GM <- shap_values_GM %>% reshape2::melt() 
#shap_value_RL <- shap_values_RL %>% reshape2::melt() 
#shap_value_RS <- shap_values_RS %>% reshape2::melt() 
shap_value_R <- shap_values_R %>% reshape2::melt() 
shap_value_W <- shap_values_W %>% reshape2::melt() 

shap_values<-rbind(shap_value_G,
                   shap_value_GM,
                  # shap_value_RL
                  #,shap_value_RS
                   shap_value_R,
                   shap_value_W
                   
                   )

shap_values
```
### Summary plots
```{r}
shap_values %>% group_by(class,variable) %>% summarise(mean=mean(abs(value))) %>% arrange(desc(mean)) %>%
  ggplot()+
  ggdark::dark_theme_classic()+
  geom_col(aes(y=variable,x=mean,group = class, fill = class),position="stack")+
  xlab("Mean(|Shap Value|) Average impact on model output magnitude")
pl<-plotly::ggplotly()
pl
#pl<-plotly::partial_bundle(pl)
#htmlwidgets::saveWidget(pl,"/tmp/plotly.html")
```

```{r fig.height=8, fig.width=12}


cbind(shap_values, feature_value=trainset_melted_scaled$value_scale) %>% # filter(class=="GM") %>%
  ggplot()+
  facet_wrap(~class)+
  ggdark::dark_theme_bw()+
 # geom_jitter(aes(y=variable,x=value, color = value), position = position_jitter(w = 0, h = 0.4),size=0.1)
  #geom_violin(aes(y=variable,x=value))+
  geom_hline(yintercept=0, 
                color = "red", size=0.5)+
  ggforce::geom_sina(aes(x=variable,y=value,color=feature_value),size=0.8,bins=4,alpha=0.5)+
    scale_colour_gradient(low = "yellow", high = "red", na.value = NA)+
   #scale_colour_gradientn(colours = terrain.colors(10))+
  xlab("Feature")+ylab("SHAP value")+
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1))
  
  #xlab("Mean(|Shap Value|) Average impact on model output magnitude")


#plotly::ggplotly()
#autoplot(shap_values, type = "contribution", row_num = 2)
#plotly::ggplotly()
```

### Dependency plots

```{r}
#features_values<-cbind(trainset,class=trainset_y) %>% reshape2::melt(id.vars="class")
#features_values %>% filter(variable=="Standing" & value >1)

p2<-cbind(shap_values, feature_value=trainset_melted_scaled$value_scale)  %>% filter(variable=="Active",class=="R" ) %>%

ggplot(aes(x=feature_value, y=value)) +
  ggdark::dark_theme_bw()+
  geom_point(alpha = 0.3,color='orange') +
  geom_smooth(color='red') +
  ylab("Shapley value")
p2
```

## python shap
```{r eval=FALSE, include=FALSE}
library(reticulate)
py_config()
```

```{python eval=FALSE, include=FALSE}
r.shap_values
# visualize the first prediction's explanation
import shap
import matplotlib

shap.initjs()
shap.plots.force(r.shap,matplotlib=True)
```

```{r eval=FALSE, include=FALSE}
library(reticulate)
py_config()
```

```{python eval=FALSE, include=FALSE}
import shap
from sklearn.ensemble import RandomForestClassifier
import matplotlib
# train an RF model
X= r.trainset
y= r.trainset_y

model = RandomForestClassifier().fit(X, y)

# explain the model's predictions using SHAP
# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X,approximate=True)

# visualize the first prediction's explanation
shap.initjs()
#shap.plots.force(explainer.expected_value[0],shap_values[0],matplotlib=True)
#shap.summary_plot(shap_values, X, plot_type="bar")
shap.summary_plot(shap_values, X,plot_size=(8,10))
#shap.summary_plot(shap_values[0], X)


```

```{python eval=FALSE, include=FALSE}
import pandas as pd
import matplotlib.pyplot as plt
df=pd.DataFrame([[1, 2], [3, 4], [4, 3], [2, 3]])
fig = plt.figure(figsize=(14,8))
for i in df.columns:
    ax=plt.subplot(2,1,i+1) 
    df[[i]].plot(ax=ax)
    print(i)

plt.show()
```

```{r eval=FALSE, include=FALSE}
data(titanic_imputed, package = "DALEX")
head(titanic_imputed)
ranger_model <- ranger::ranger(survived~., data = titanic_imputed, classification = TRUE, probability = TRUE)
pred_fun <- function(X.model, newdata) {
  predict(X.model, newdata)$predictions[,2]
}

shap <- fastshap::explain(ranger_model, X = titanic_imputed, pred_wrapper = pred_fun, nsim = 50)
library(ggplot2)
autoplot(shap)
autoplot(shap, type = "dependence", feature = "fare", X = titanic_imputed, smooth = TRUE)+ ggdark::dark_theme_bw()

shap %>% group_by(survived) %>% summarise(n=n())
```

